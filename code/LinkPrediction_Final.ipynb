{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://128.232.240.137:4040\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1642502821365)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkContext\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.mllib.tree.DecisionTree\n",
       "import org.apache.spark.mllib.tree.RandomForest\n",
       "import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
       "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import java.nio.file.{Paths, Files}\n",
       "import java.nio.charset.StandardCharsets\n",
       "import scala.util.control.Breaks._\n",
       "import scala.math._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import java.nio.file.{Paths, Files}\n",
    "import java.nio.charset.StandardCharsets\n",
    "import scala.util.control.Breaks._\n",
    "import scala.math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jobName: String = siteRequest\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7c073a91\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@157101dc\n",
       "res0: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7c073a91\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jobName = \"siteRequest\"\n",
    "val conf = new SparkConf().setAppName(jobName).setMaster(\"local[*]\").set(\"spark.executor.memory\",\"10g\").set(\"spark.driver.memory\",\"10g\")\n",
    "val sc =  SparkContext.getOrCreate(conf)\n",
    "conf.set(\"spark.streaming.stopGracefullyOnShutdown\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RDData: org.apache.spark.rdd.RDD[String] = com-dblp.ungraph.txt MapPartitionsRDD[1] at textFile at <console>:47\n",
       "RDData2: org.apache.spark.rdd.RDD[String] = unconnected.txt MapPartitionsRDD[3] at textFile at <console>:48\n",
       "RDData3: org.apache.spark.rdd.RDD[String] = 2hop_final.txt MapPartitionsRDD[5] at textFile at <console>:49\n",
       "Splitted1RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[6] at map at <console>:50\n",
       "Splitted2RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[7] at map at <console>:51\n",
       "Splitted3RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[8] at map at <console>:52\n",
       "EdgeRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[9] at map at <console>:53\n",
       "Vertex1RDD: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[13] at distinct at...\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val RDData = sc.textFile(\"com-dblp.ungraph.txt\");\n",
    "val RDData2 = sc.textFile(\"unconnected.txt\");\n",
    "val RDData3 = sc.textFile(\"2hop_final.txt\");\n",
    "val Splitted1RDD = RDData.map(_.split(\"\\t\"));\n",
    "val Splitted2RDD = RDData2.map(_.split(\" \"));\n",
    "val Splitted3RDD = RDData3.map(_.split(\" \"));\n",
    "val EdgeRDD = Splitted1RDD.map(row=>(row(0).toLong,row(1).toLong));\n",
    "val Vertex1RDD = EdgeRDD.map{t => t._1}.distinct();\n",
    "val Vertex2RDD = EdgeRDD.map{t => t._2}.distinct();\n",
    "val VertexRDD = Vertex1RDD.union(Vertex2RDD).distinct();\n",
    "\n",
    "\n",
    "val UnConnectedEdgeRDD = Splitted2RDD.map(row=>(row(0).toLong,row(1).toLong));\n",
    "val twoHopEdgeRDD = Splitted3RDD.map(row=>(row(0).toLong,row(1).toLong));\n",
    "// EdgeRDD.take(2).foreach(arr => println(arr.toList))\n",
    "// UnConnectedEdgeRDD.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// println(\"Total Number of vertex: \" + VertexRDD.count())\n",
    "// println(\"Total Number of Connected Edge: \" + EdgeRDD.count())\n",
    "// println(\"Total Number of Unconnected Edge: \" + UnConnectedEdgeRDD.count())\n",
    "// println(\"Total Number of TwoHop Edge: \" + twoHopEdgeRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deltaRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[27] at subtract at <console>:50\n",
       "interRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[33] at intersection at <console>:51\n",
       "deltaSmallRDD: org.apache.spark.rdd.RDD[(Long, Long)] = ParallelCollectionRDD[35] at parallelize at <console>:52\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val deltaRDD = twoHopEdgeRDD.subtract(EdgeRDD)\n",
    "val interRDD = twoHopEdgeRDD.intersection(EdgeRDD)\n",
    "val deltaSmallRDD = sc.parallelize(deltaRDD.sample(false,0.1,0).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// deltaRDD.count()\n",
    "// interRDD.count()\n",
    "// deltaSmallRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Validation Graph = unConnectedEdge + connectedEdge\n",
    "// Test Graph = (unConnectedEdge + 0.1*connectedEdge) + 0.9*connectedEdge\n",
    "// Train Graph = ((unConnectedEdge + 0.1*connectedEdge) + 0.1*0.9*connectedEdge) + 0.9*0.9*connectedEdge\n",
    "\n",
    "// What we want to predict for testgraph is 0.1*connectedEdge\n",
    "// What we want to predict for traingraph is 0.1*0.9*connectedEdge\n",
    "\n",
    "// test set is unConnectedEdge (label 0) + 0.1*connectedEdge (label 1)\n",
    "// train set is unConnectedEdge + 0.1*connectedEdge (label 0) + 0.1*0.9*connectedEdge (label 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[36] at randomSplit at <console>:52\n",
       "bRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[37] at randomSplit at <console>:52\n",
       "testSetPos: org.apache.spark.rdd.RDD[(Long, Long, Int)] = MapPartitionsRDD[38] at map at <console>:55\n",
       "testSetNeg: org.apache.spark.rdd.RDD[(Long, Long, Int)] = ParallelCollectionRDD[41] at parallelize at <console>:57\n",
       "testSet: org.apache.spark.rdd.RDD[(Long, Long, Int)] = UnionRDD[42] at union at <console>:59\n",
       "cRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[43] at randomSplit at <console>:62\n",
       "dRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[44] at randomSplit at <console>:62\n",
       "trainSetPos: org.apache.spark.rdd.RDD[(Long, Long, Int)] = MapPartitionsRDD[45] at map at <...\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Construct Test Set with Validation Graph and Testing Graph\n",
    "//Missing Link for test graph: aRDD\n",
    "val Array(aRDD, bRDD) = interRDD.randomSplit(Array(0.12, 0.88));\n",
    "// TestSet positive: testSetPos\n",
    "// val testSetPos = sc.parallelize(aRDD.map{t=>(t._1,t._2,1)}.sample(false,0.8,0).collect());\n",
    "val testSetPos = aRDD.map{t=>(t._1,t._2,1)};\n",
    "//TestSet negative: testSetNeg\n",
    "var testSetNeg = sc.parallelize(deltaSmallRDD.map{t=>(t._1,t._2,0)}.sample(false,0.14,0).collect());\n",
    "// var testSetNeg = UnConnectedEdgeRDD.map{t=>(t._1,t._2,0)};\n",
    "var testSet = testSetPos.union(testSetNeg);\n",
    "//Construct Train Set with Testing Graph and Training Graph\n",
    "//Missing Link for train graph: cRDD\n",
    "val Array(cRDD, dRDD) = bRDD.randomSplit(Array(0.25, 0.75));\n",
    "// val trainSetPos = sc.parallelize(cRDD.map{t=>(t._1,t._2,1)}.sample(false,0.7,0).collect());\n",
    "val trainSetPos = cRDD.map{t=>(t._1,t._2,1)};\n",
    "val trainSetNeg = sc.parallelize(aRDD.union(deltaSmallRDD).map{t=>(t._1,t._2,0)}.sample(false,0.28,0).collect());\n",
    "var trainSet = trainSetPos.union(trainSetNeg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// println(testSetPos.count())\n",
    "// println(testSetNeg.count())\n",
    "// println(testSet.count())\n",
    "// println(trainSetPos.count)\n",
    "// println(trainSetNeg.count)\n",
    "// println(trainSet.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphVertexRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[54] at distinct at <console>:54\n",
       "nAllVertex: Long = 317080\n",
       "TestGraphEdgeRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[58] at distinct at <console>:56\n",
       "testGraph: org.apache.spark.graphx.Graph[Long,Int] = org.apache.spark.graphx.impl.GraphImpl@7fd410de\n",
       "testNeigh: org.apache.spark.graphx.VertexRDD[Array[org.apache.spark.graphx.VertexId]] = VertexRDDImpl[76] at RDD at VertexRDD.scala:57\n",
       "testBroadcastVar: org.apache.spark.broadcast.Broadcast[Array[(org.apache.spark.graphx.VertexId, Array[org.apache.spark.graphx.VertexId])]] = Broadcast(20)\n",
       "testCCBroadcastVar: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,org.apache.spark.graphx....\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Build similarity indices for each edge in testSet\n",
    "//Build Test Graph\n",
    "val GraphVertexRDD = VertexRDD.map(line=>(line,line)).distinct();\n",
    "val nAllVertex = VertexRDD.count();\n",
    "val TestGraphEdgeRDD = bRDD.map{t=>(Edge(t._1,t._2,1))}.distinct()\n",
    "val testGraph = Graph(GraphVertexRDD,TestGraphEdgeRDD);\n",
    "val testNeigh = testGraph.collectNeighborIds(EdgeDirection.Either)\n",
    "val testBroadcastVar = sc.broadcast(testNeigh.collect())\n",
    "// val testNValues = testBroadcastVar.value.toMap\n",
    "val testCCBroadcastVar = sc.broadcast(testGraph.connectedComponents.vertices.collect().toMap)\n",
    "\n",
    "\n",
    "val testFeature = testSet.mapPartitions(rows=>{\n",
    "    val testNValues = testBroadcastVar.value.toMap\n",
    "    val ccTest = testCCBroadcastVar.value\n",
    "    rows.map(t=>{\n",
    "        val n1_neigh = testNValues(t._1)\n",
    "        val n2_neigh = testNValues(t._2)\n",
    "        val commonNei = n1_neigh.intersect(n2_neigh)\n",
    "        //Common neigbors\n",
    "        val CN = commonNei.length\n",
    "        //Jaccard’s Coefficient \n",
    "        val JC = n1_neigh.intersect(n2_neigh).size/n1_neigh.union(n2_neigh).size.toDouble\n",
    "        //Adamic/Adar\n",
    "        var AA = 0.0\n",
    "        commonNei.foreach(x => AA += 1/Math.log10(testNValues(x).length))\n",
    "        //Resource Allocation\n",
    "        var RA = 0.0\n",
    "        commonNei.foreach(x => RA += 1/testNValues(x).length.toDouble)\n",
    "        //Preferential Attachment\n",
    "        val PA = n1_neigh.length*n2_neigh.length\n",
    "        //Adjusted Rand\n",
    "        val a = commonNei.length\n",
    "        val b = n1_neigh.toSet.diff(n2_neigh.toSet).size\n",
    "        val c = n2_neigh.toSet.diff(n1_neigh.toSet).size\n",
    "        val d = nAllVertex - n1_neigh.union(n2_neigh).length\n",
    "        val AR = 2*(a*d-b*c)/((a+b)*(b+d)+(a+c)*(c+d)).toDouble\n",
    "        //Neighborhood Distance\n",
    "        val nd = commonNei.length / Math.sqrt(n1_neigh.length*n2_neigh.length).toDouble\n",
    "        val ND = if (nd.isNaN) 0 else nd\n",
    "        //Total Neighbours\n",
    "        val TN = n1_neigh.union(n2_neigh).size\n",
    "        //Node Degree U\n",
    "        val UND = n1_neigh.length\n",
    "        //Node Degree V\n",
    "        val VND = n2_neigh.length\n",
    "        //Same Community\n",
    "        val SC = if (ccTest(t._1)==ccTest(t._2)) 1 else 0\n",
    "        (t._1,t._2,t._3,CN,JC,AA,RA,PA,AR,ND,TN,UND,VND,SC)\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphVertexRDD: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[408] at distinct at <console>:57\n",
       "nAllVertex: Long = 317080\n",
       "TrainGraphEdgeRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[412] at distinct at <console>:59\n",
       "trainGraph: org.apache.spark.graphx.Graph[Long,Int] = org.apache.spark.graphx.impl.GraphImpl@67b6021e\n",
       "trainNeigh: org.apache.spark.graphx.VertexRDD[Array[org.apache.spark.graphx.VertexId]] = VertexRDDImpl[430] at RDD at VertexRDD.scala:57\n",
       "trainBroadcastVar: org.apache.spark.broadcast.Broadcast[Array[(org.apache.spark.graphx.VertexId, Array[org.apache.spark.graphx.VertexId])]] = Broadcast(106)\n",
       "trainCCBroadcastVar: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,org.apache.spar...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Build similarity indices for each edge in trainset\n",
    "//Build Train Graph\n",
    "val GraphVertexRDD = VertexRDD.map(line=>(line,line)).distinct();\n",
    "val nAllVertex = VertexRDD.count();\n",
    "val TrainGraphEdgeRDD = dRDD.map{t=>(Edge(t._1,t._2,1))}.distinct()\n",
    "val trainGraph = Graph(GraphVertexRDD,TrainGraphEdgeRDD);\n",
    "val trainNeigh = trainGraph.collectNeighborIds(EdgeDirection.Either)\n",
    "val trainBroadcastVar = sc.broadcast(trainNeigh.collect())\n",
    "// val trainNValues = trainBroadcastVar.value.toMap\n",
    "val trainCCBroadcastVar = sc.broadcast(trainGraph.connectedComponents.vertices.collect().toMap)\n",
    "\n",
    "\n",
    "\n",
    "val trainFeature = trainSet.mapPartitions(rows=>{\n",
    "    val trainNValues = trainBroadcastVar.value.toMap\n",
    "    val ccTrain = trainCCBroadcastVar.value\n",
    "    rows.map(t=>{\n",
    "        val n1_neigh = trainNValues(t._1)\n",
    "        val n2_neigh = trainNValues(t._2)\n",
    "        val commonNei = n1_neigh.intersect(n2_neigh)\n",
    "        //Common neigbors\n",
    "        val CN = commonNei.length\n",
    "        //Jaccard’s Coefficient \n",
    "        val JC = n1_neigh.intersect(n2_neigh).size/n1_neigh.union(n2_neigh).size.toDouble\n",
    "        //Adamic/Adar\n",
    "        var AA = 0.0\n",
    "        commonNei.foreach(x => AA += 1/Math.log10(trainNValues(x).length))\n",
    "        //Resource Allocation\n",
    "        var RA = 0.0\n",
    "        commonNei.foreach(x => RA += 1/trainNValues(x).length.toDouble)\n",
    "        //Preferential Attachment\n",
    "        val PA = n1_neigh.length*n2_neigh.length\n",
    "        //Adjusted Rand\n",
    "        val a = commonNei.length\n",
    "        val b = n1_neigh.toSet.diff(n2_neigh.toSet).size\n",
    "        val c = n2_neigh.toSet.diff(n1_neigh.toSet).size\n",
    "        val d = nAllVertex - n1_neigh.union(n2_neigh).length\n",
    "        val AR = 2*(a*d-b*c)/((a+b)*(b+d)+(a+c)*(c+d)).toDouble\n",
    "        //Neighborhood Distance\n",
    "        val nd = commonNei.length / Math.sqrt(n1_neigh.length*n2_neigh.length).toDouble\n",
    "        val ND = if (nd.isNaN) 0 else nd\n",
    "        //Total Neighbours\n",
    "        val TN = n1_neigh.union(n2_neigh).size\n",
    "        //Node Degree U\n",
    "        val UND = n1_neigh.length\n",
    "        //Node Degree V\n",
    "        val VND = n2_neigh.length\n",
    "        //Same Community\n",
    "        val SC = if (ccTrain(t._1)==ccTrain(t._2)) 1 else 0\n",
    "        (t._1,t._2,t._3,CN,JC,AA,RA,PA,AR,ND,TN,UND,VND,SC)\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsedTestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[827] at map at <console>:49\n",
       "parsedTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[828] at map at <console>:53\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsedTestData = testFeature.map{x => \n",
    "    val parts1 = Array(x._1,x._2,x._4,x._5,x._6,x._7,x._8,x._9,x._10,x._11,x._12,x._13,x._14)\n",
    "    LabeledPoint(x._3.toDouble, Vectors.dense(parts1.map(_.toDouble)))\n",
    "}.cache();\n",
    "val parsedTrainData = trainFeature.map{x => \n",
    "    val parts1 = Array(x._1,x._2,x._4, x._5,x._6,x._7,x._8,x._9,x._10,x._11,x._12,x._13,x._14)\n",
    "    LabeledPoint(x._3.toDouble, Vectors.dense(parts1.map(_.toDouble)))\n",
    "}.cache();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 277422\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedTestData.count()\n",
    "parsedTrainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses: Int = 2\n",
       "categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n",
       "impurity: String = gini\n",
       "maxDepth: Int = 10\n",
       "maxBins: Int = 32\n",
       "model_DT: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 10 with 1179 nodes\n",
       "predictions_DT: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[869] at map at <console>:55\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Decision Tree\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 10\n",
    "val maxBins = 32\n",
    "val model_DT = DecisionTree.trainClassifier(parsedTrainData, numClasses, categoricalFeaturesInfo,impurity, maxDepth, maxBins)\n",
    "val predictions_DT = parsedTestData.map(p => (model_DT.predict(p.features), p.label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses: Int = 2\n",
       "categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n",
       "numTrees: Int = 10\n",
       "featureSubsetStrategy: String = auto\n",
       "impurity: String = gini\n",
       "maxDepth: Int = 10\n",
       "maxBins: Int = 32\n",
       "model_RF: org.apache.spark.mllib.tree.model.RandomForestModel =\n",
       "TreeEnsembleModel classifier with 10 trees\n",
       "\n",
       "predictions_RF: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[910] at map at <console>:64\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Random Forest\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val numTrees = 10 // Use more in practice.\n",
    "val featureSubsetStrategy = \"auto\" // Let the algorithm choose.\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 10\n",
    "val maxBins = 32\n",
    "val model_RF = RandomForest.trainClassifier(parsedTrainData, numClasses, categoricalFeaturesInfo,\n",
    "  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n",
    "val predictions_RF = parsedTestData.map(p => (model_RF.predict(p.features), p.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boostingStrategy: org.apache.spark.mllib.tree.configuration.BoostingStrategy = BoostingStrategy(org.apache.spark.mllib.tree.configuration.Strategy@5467042d,org.apache.spark.mllib.tree.loss.LogLoss$@69d0bee9,15,0.1,0.001)\n",
       "boostingStrategy.numIterations: Int = 15\n",
       "boostingStrategy.treeStrategy.maxDepth: Int = 10\n",
       "boostingStrategy.treeStrategy.categoricalFeaturesInfo: Map[Int,Int] = Map()\n",
       "model_GB: org.apache.spark.mllib.tree.model.GradientBoostedTreesModel =\n",
       "TreeEnsembleModel classifier with 15 trees\n",
       "\n",
       "predictions_GB: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1473] at map at <console>:53\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n",
    "boostingStrategy.numIterations = 15\n",
    "boostingStrategy.treeStrategy.maxDepth = 10\n",
    "boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val model_GB = GradientBoostedTrees.train(parsedTrainData, boostingStrategy)\n",
    "val predictions_GB = parsedTestData.map(p => (model_GB.predict(p.features), p.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT & 79.446 & 0.867 & 0.744 & 0.801\n",
      "RF & 79.784 & 0.857 & 0.763 & 0.807\n",
      "GB & 79.703 & 0.864 & 0.753 & 0.805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_supervised: Array[org.apache.spark.rdd.RDD[(Double, Double)]] = Array(MapPartitionsRDD[869] at map at <console>:55, MapPartitionsRDD[910] at map at <console>:64, MapPartitionsRDD[1473] at map at <console>:53)\n",
       "acc_sup_arr: Array[Double] = Array(79.44565717764809, 79.78423248838004, 79.70330110927964)\n",
       "roc_sup_arr: Array[Array[Array[Double]]] = Array(Array(Array(0.0, 0.0), Array(0.1428929179605329, 0.7442768506632434), Array(1.0, 1.0), Array(1.0, 1.0)), Array(Array(0.0, 0.0), Array(0.15865304350729573, 0.762997432605905), Array(1.0, 1.0), Array(1.0, 1.0)), Array(Array(0.0, 0.0), Array(0.14823533340011352, 0.7531958707744972), Array(1.0, 1.0), Array(1.0, 1.0)))\n",
       "auroc_sup_arr: Array[Double] = Array(0.8006919663513553, 0.8021721945493047, 0.8024802686871918)\n",
       "prec_sup_arr: Array[Do...\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var predictions_supervised = Array(predictions_DT,predictions_RF,predictions_GB)\n",
    "\n",
    "var acc_sup_arr :  Array[Double] = Array()\n",
    "var roc_sup_arr : Array[Array[Array[Double]]] = Array()\n",
    "var auroc_sup_arr :  Array[Double] = Array()\n",
    "var prec_sup_arr: Array[Double] = Array()\n",
    "var recall_sup_arr: Array[Double] = Array()\n",
    "var f1_sup_arr: Array[Double] = Array()\n",
    "val name = Array(\"DT\",\"RF\",\"GB\")\n",
    "var count = 0\n",
    "for(p <- predictions_supervised){\n",
    "    val accuracy = 100.0 * p.filter(x => x._1 == x._2).count() / (parsedTestData.count())\n",
    "    val positively_Predicted = p.filter(x => x._1 == x._2).count()\n",
    "//     println(\"Total Positive Prediction Correctly = \" + positively_Predicted)\n",
    "//     println(\"Accuracy = \" + accuracy)\n",
    "    var a = (accuracy* 1000).round / 1000.toDouble\n",
    "    acc_sup_arr :+= accuracy\n",
    "    \n",
    "    val metrics = new BinaryClassificationMetrics(p)\n",
    "    val roc = metrics.roc.collect().map(t=>Array(t._1,t._2))\n",
    "    roc_sup_arr :+= roc\n",
    "    \n",
    "    val auROC = metrics.areaUnderROC\n",
    "    auroc_sup_arr :+= auROC\n",
    "//     println(\"Area under ROC = \" + auROC)\n",
    "    \n",
    "    // Precision by threshold\n",
    "    val precision = metrics.precisionByThreshold\n",
    "    val precHighest = precision.collect()\n",
    "    prec_sup_arr :+= precHighest(0)._2\n",
    "    var pre = (precHighest(0)._2* 1000).round / 1000.toDouble\n",
    "\n",
    "    // Recall by threshold\n",
    "    val recall = metrics.recallByThreshold\n",
    "    val recallHighest = recall.collect()\n",
    "    recall_sup_arr :+= recallHighest(0)._2\n",
    "    var r = (recallHighest(0)._2* 1000).round / 1000.toDouble\n",
    "\n",
    "    // F1-score\n",
    "    val f1Score = metrics.fMeasureByThreshold\n",
    "    val f1Highest = f1Score.collect()\n",
    "    f1_sup_arr :+= f1Highest(0)._2\n",
    "    var f1 = (f1Highest(0)._2* 1000).round / 1000.toDouble\n",
    "    \n",
    "    var n = name(count)\n",
    "    count+=1\n",
    "    \n",
    "    \n",
    "    println(s\"$n & $a & $pre & $r & $f1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions_CN: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[176425] at map at <console>:60\n",
       "predictions_JC: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[176426] at map at <console>:61\n",
       "predictions_AA: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[176427] at map at <console>:62\n",
       "predictions_RA: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[176428] at map at <console>:63\n",
       "predictions_PA: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[176429] at map at <console>:64\n",
       "predictions_AR: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[176430] at map at <console>:65\n",
       "predictions_ND: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[176431] at map at <console>:66\n",
       "predictions_TN: org.apache.spark.r...\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Unsupervised\n",
    "val predictions_CN = parsedTestData.map(p => (p.features(2), p.label))\n",
    "val predictions_JC = parsedTestData.map(p => (p.features(3), p.label))\n",
    "val predictions_AA = parsedTestData.map(p => (p.features(4), p.label))\n",
    "val predictions_RA = parsedTestData.map(p => (p.features(5), p.label))\n",
    "val predictions_PA = parsedTestData.map(p => (p.features(6), p.label))\n",
    "val predictions_AR = parsedTestData.map(p => (p.features(7), p.label))\n",
    "val predictions_ND = parsedTestData.map(p => (p.features(8), p.label))\n",
    "val predictions_TN = parsedTestData.map(p => (p.features(9), p.label))\n",
    "val predictions_UND = parsedTestData.map(p => (p.features(10), p.label))\n",
    "val predictions_VND = parsedTestData.map(p => (p.features(11), p.label))\n",
    "val predictions_SC = parsedTestData.map(p => (p.features(12), p.label))\n",
    "\n",
    "// var predictions_unsupervised = Array(predictions_CN)\n",
    "var predictions_unsupervised = Array(predictions_CN,predictions_JC,predictions_AA,predictions_RA,predictions_PA,predictions_AR,predictions_ND,predictions_TN,predictions_UND,predictions_VND,predictions_SC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.7358160137879454\n",
      "(1.0,0.408999)\n",
      "Area under ROC = 0.7467962074268947\n",
      "(0.043795620437956206,0.44389284)\n",
      "Area under ROC = 0.7561505153585301\n",
      "(1.1061923157604907,0.48860496)\n",
      "Area under ROC = 0.757853099189961\n",
      "(0.08382352941176471,0.49218893)\n",
      "Area under ROC = 0.4548192595246393\n",
      "(2472.0,0.0221174)\n",
      "Area under ROC = 0.7935376164565342\n",
      "(0.09089423397820755,0.44413823)\n",
      "Area under ROC = 0.7459152291931872\n",
      "(0.13608276348795434,0.45470116)\n",
      "Area under ROC = 0.42353003830570585\n",
      "(166.0,0.018414682)\n",
      "Area under ROC = 0.44088407415788267\n",
      "(0.0,3.579259E-4)\n",
      "Area under ROC = 0.4791516427082723\n",
      "(74.0,0.0071356315)\n",
      "Area under ROC = 0.48382696133276804\n",
      "(1.0,0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roc_unsup_arr: Array[Array[Array[Double]]] = Array(Array(Array(0.0, 0.0), Array(0.0, 2.6743688489516474E-5), Array(0.0, 4.011553273427471E-5), Array(0.0, 6.685922122379119E-5), Array(0.0, 8.023106546854942E-5), Array(0.0, 1.2034659820282414E-4), Array(0.0, 1.6046213093709883E-4), Array(0.0, 1.8720581942661531E-4), Array(0.0, 2.0057766367137357E-4), Array(0.0, 2.139495079161318E-4), Array(0.0, 2.5406504065040653E-4), Array(0.0, 2.941805733846812E-4), Array(0.0, 3.7441163885323063E-4), Array(0.0, 4.1452717158750536E-4), Array(0.0, 4.278990158322636E-4), Array(0.0, 4.6801454856653827E-4), Array(0.0, 4.947582370560548E-4), Array(0.0, 5.081300813008131E-4), Array(0.0, 5.482456140350877E-4), Array(0.0, 5.749893025246041E-4), Array(0.0, 6.017329910141207E-4), Array(0.0, 6.284766795036372E-4), ...\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var roc_unsup_arr : Array[Array[Array[Double]]] = Array()\n",
    "var auroc_unsup_arr :  Array[Double] = Array()\n",
    "\n",
    "for(p <- predictions_unsupervised){\n",
    "    val metrics = new BinaryClassificationMetrics(p)\n",
    "    val roc = metrics.roc.collect().map(t=>Array(t._1,t._2))\n",
    "    roc_unsup_arr :+= roc\n",
    "    val auROC = metrics.areaUnderROC\n",
    "    auroc_unsup_arr :+= auROC\n",
    "    println(\"Area under ROC = \" + auROC)\n",
    "     val truePos = p.filter{case (a,b) => b==1}.count()\n",
    "    val trueNeg = p.filter{case (a,b) => b==0}.count()\n",
    "    \n",
    "    val predArr = p.collect()\n",
    "    val precision = metrics.precisionByThreshold\n",
    "    \n",
    "    val thresholds = precision.map(_._1).collect()\n",
    "    \n",
    "    val tpr_fpr = thresholds.map(t => {\n",
    "      var tp = 0;\n",
    "      var fp = 0;\n",
    "//       println(t)\n",
    "        \n",
    "      predArr.map(r => {\n",
    "\n",
    "        if (r._2 == 1 && r._1 > t){\n",
    "            tp=tp+1;\n",
    "\n",
    "        } // true positive\n",
    "        if (r._2 == 0 && r._1 > t){\n",
    "            fp=fp+1;\n",
    "        }\n",
    "      })\n",
    "\n",
    "      (t,tp.toFloat/truePos-fp.toFloat/trueNeg)\n",
    "    })\n",
    "    \n",
    "    println(tpr_fpr.maxBy(_._2))\n",
    "\n",
    "    \n",
    "    //     metrics.roc.collect.foreach { x =>\n",
    "//       println(x)\n",
    "//     }\n",
    "    \n",
    "    \n",
    "    \n",
    "//     // Precision by threshold\n",
    "//     val precision = metrics.precisionByThreshold\n",
    "//     precision.collect.foreach { case (t, p) =>\n",
    "//       println(s\"Threshold: $t, Precision: $p\")\n",
    "//     }\n",
    "//     // Recall by threshold\n",
    "//     val recall = metrics.recallByThreshold\n",
    "//     recall.collect.foreach { case (t, r) =>\n",
    "//       println(s\"Threshold: $t, Recall: $r\")\n",
    "//     }\n",
    "//     val joinRDD = precision.join(recall).map{case (a,(c,d))=>(a,c-d)}\n",
    "// //     println(joinRDD)\n",
    "//     joinRDD.collect.foreach { t =>\n",
    "//       println(t)\n",
    "//     }\n",
    "//     println(joinRDD.maxBy(_._2))\n",
    "//     joinRDD.reduce((a,b)=> a._1 + b._1)\n",
    "    // Compute thresholds used in ROC and PR curves\n",
    "//     val max\n",
    "//     val thresholds = precision.map(_._1)\n",
    "//     thresholds.filter{thr=>joinRDD}\n",
    "//     thresholds.collect.foreach { thr =>\n",
    "//         var p = precision.filter{case (t, p) => t==thr}\n",
    "//         var r = recall.filter{case (t, p) => t==thr}\n",
    "        \n",
    "//     }\n",
    "\n",
    "//     inputRDD.reduce( )\n",
    "//     (a,b)=> (\"max\",a._2 min b._2)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "//     // F-measure\n",
    "//     val f1Score = metrics.fMeasureByThreshold\n",
    "// //     f1Score.collect.foreach { case (t, f) =>\n",
    "// //       println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n",
    "// //     }\n",
    "//     println(f1Score.collect.maxBy(_._2))\n",
    "}\n",
    "\n",
    "//Pa 4, tn 7, und 8, vnd 9, sc 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestT: Array[Double] = Array(1.0, 0.043795620437956206, 1.1061923157604907, 0.08382352941176471, 2472.0, 0.09089423397820755, 0.13608276348795434, 166.0, 0.0, 74.0, 1.0)\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CN=1.0,JC=0.043795620437956206,AA=1.1061923157604907,RA=,PA,AR,ND,TN,UND,VND,SC\n",
    "val bestT = Array(1.0,0.043795620437956206,1.1061923157604907,0.08382352941176471,2472.0,0.09089423397820755,0.13608276348795434,166.0,0.0,74.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN & 67.756 & 0.723 & 0.664 & 0.693 & 1.0\n",
      "JC & 70.505 & 0.85 & 0.569 & 0.682 & 0.043795620437956206\n",
      "AA & 72.364 & 0.91 & 0.557 & 0.691 & 1.1061923157604907\n",
      "RA & 72.503 & 0.916 & 0.556 & 0.692 & 0.08382352941176471\n",
      "PA & 45.893 & 0.74 & 0.039 & 0.075 & 2472.0\n",
      "AR & 70.469 & 0.853 & 0.565 & 0.68 & 0.09089423397820755\n",
      "ND & 70.986 & 0.859 & 0.571 & 0.686 & 0.13608276348795434\n",
      "TN & 45.574 & 0.814 & 0.026 & 0.05 & 166.0\n",
      "UND & 55.267 & 0.555 & 1.0 & 0.714 & 0.0\n",
      "VND & 45.074 & 0.631 & 0.026 & 0.05 & 74.0\n",
      "SC & 44.474 & 0.546 & 0.844 & 0.663 & 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name: Array[String] = Array(CN, JC, AA, RA, PA, AR, ND, TN, UND, VND, SC)\n",
       "count: Int = 11\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val name = Array(\"CN\",\"JC\",\"AA\",\"RA\",\"PA\",\"AR\",\"ND\",\"TN\",\"UND\",\"VND\",\"SC\")\n",
    "var count = 0\n",
    "for(p <- predictions_unsupervised){\n",
    "    val metrics = new BinaryClassificationMetrics(p)\n",
    "    val roc = metrics.roc.collect().map(t=>Array(t._1,t._2))\n",
    "    roc_unsup_arr :+= roc\n",
    "    val auROC = metrics.areaUnderROC\n",
    "    auroc_unsup_arr :+= auROC\n",
    "//     println(\"Area under ROC = \" + auROC)\n",
    "   \n",
    "    val best = bestT(count)\n",
    "    \n",
    "    // Precision by threshold\n",
    "    val precision = metrics.precisionByThreshold\n",
    "    val preFilter = precision.filter{case(t,r)=>t==best}.collect()\n",
    "    val pre = (preFilter(0)._2* 1000).round / 1000.toDouble\n",
    "//     val pre = preFilter(0)._2\n",
    "//     val pre = 1\n",
    "    \n",
    "    // Recall by threshold\n",
    "    val recall = metrics.recallByThreshold\n",
    "    val reFilter = recall.filter{case(t,r)=>t==best}.collect()\n",
    "    val r = (reFilter(0)._2* 1000).round / 1000.toDouble\n",
    "//     val r = reFilter(0)._2\n",
    "//     val r = 1\n",
    "    //F-measure\n",
    "    val f1Score = metrics.fMeasureByThreshold\n",
    "    val f1Filter = f1Score.filter{case(t,r)=>t==best}.collect()\n",
    "    val f = (f1Filter(0)._2* 1000).round / 1000.toDouble\n",
    "//     val f = f1Filter(0)._2\n",
    "//     val f = 1\n",
    "    //Accuracy\n",
    "    var a = 100.0 * p.filter(x => {\n",
    "        val predicted = if(x._1>best) 1 else 0\n",
    "         predicted == x._2\n",
    "    }).count() / (parsedTestData.count())\n",
    "    a = (a* 1000).round / 1000.toDouble\n",
    "    \n",
    "    val n = name(count)\n",
    "//     println(s\"Threshold: $best, Accuracy: $a, Precision: $pre, Recall: $r, F1-Score: $f\")\n",
    "    println(s\"$n & $a & $pre & $r & $f & $best\")\n",
    "//     val joinRDD = precision.join(recall).map{case (a,(c,d))=>(a,c-d)}\n",
    "// //     println(joinRDD)\n",
    "//     joinRDD.collect.foreach { t =>\n",
    "//       println(t)\n",
    "//     }\n",
    "//     println(joinRDD.maxBy(_._2))\n",
    "//     joinRDD.reduce((a,b)=> a._1 + b._1)\n",
    "    // Compute thresholds used in ROC and PR curves\n",
    "//     val max\n",
    "//     val thresholds = precision.map(_._1)\n",
    "//     thresholds.filter{thr=>joinRDD}\n",
    "//     thresholds.collect.foreach { thr =>\n",
    "//         var p = precision.filter{case (t, p) => t==thr}\n",
    "//         var r = recall.filter{case (t, p) => t==thr}\n",
    "        \n",
    "//     }\n",
    "\n",
    "//     inputRDD.reduce( )\n",
    "//     (a,b)=> (\"max\",a._2 min b._2)\n",
    "    \n",
    " count=count+1\n",
    "\n",
    "//     println(f1Score.collect.maxBy(_._2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|             1|    1|[3.0,0.25,2.96759...|\n",
      "|             1|    1|[1.0,0.2,0.756304...|\n",
      "|             0|    1|[1.0,0.0256410256...|\n",
      "|             1|    1|[5.0,0.3846153846...|\n",
      "|             1|    1|[28.0,0.345679012...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.7787848221437683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
       "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [n1: bigint, n2: bigint ... 12 more fields]\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [n1: bigint, n2: bigint ... 12 more fields]\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_84cb5fc752ae, handleI...\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Convert to Dataframe and ML Pipeline to use Feature Importance\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "\n",
    "val testing = testFeature.toDF(\"n1\",\"n2\",\"label\",\"CN\",\"JC\",\"AA\",\"RA\",\"PA\",\"AR\",\"ND\",\"TN\",\"UND\",\"VND\",\"SC\").cache()\n",
    "val training = trainFeature.toDF(\"n1\",\"n2\",\"label\",\"CN\",\"JC\",\"AA\",\"RA\",\"PA\",\"AR\",\"ND\",\"TN\",\"UND\",\"VND\",\"SC\").cache()\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"CN\",\"JC\",\"AA\",\"RA\",\"PA\",\"AR\",\"ND\",\"TN\",\"UND\",\"VND\",\"SC\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val trainingData = assembler.setHandleInvalid(\"keep\").transform(training)\n",
    "\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"label\")\n",
    "  .setOutputCol(\"indexedLabel\")\n",
    "  .fit(trainingData)\n",
    "// // Automatically identify categorical features, and index them.\n",
    "// // Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "val featureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .fit(trainingData)\n",
    "\n",
    "// Train a RandomForest model.\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")\n",
    "  .setNumTrees(10)\n",
    "  .setMaxDepth(10)\n",
    "\n",
    "\n",
    "// Convert indexed labels back to original labels.\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(labelIndexer.labels)\n",
    "\n",
    "// Chain indexers and forest in a Pipeline\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))\n",
    "\n",
    "// Train model.  This also runs the indexers.\n",
    "val model = pipeline.fit(trainingData)\n",
    "\n",
    "val testingData = assembler.setHandleInvalid(\"keep\").transform(testing)\n",
    "// Make predictions.\n",
    "val predictions = model.transform(testingData)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(accuracy)\n",
    "\n",
    "val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]\n",
    "// println(\"Learned classification forest model:\\n\" + rfModel.toDebugString)\n",
    "\n",
    "rfModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.ml.linalg.Vector = (11,[0,1,2,3,4,5,6,7,8,9,10],[0.00621919918917448,0.06070710416989457,0.2811441020795655,0.35530048229073624,0.06342308680438571,0.06666146882098178,0.006187987373674058,0.05658488837683489,0.02008176229570667,0.07552673892840452,0.008163179670641412])\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: String =\n",
       "79.86990832764708,80.01082861126768,80.11614798113152\n",
       "0.0,0.0,0.1497044976459982,0.757453623381823,1.0,1.0,1.0,1.0,0.0,0.0,0.1574343049851414,0.7661684238622715,1.0,1.0,1.0,1.0,0.0,0.0,0.14998831346622593,0.7621113038836247,1.0,1.0,1.0,1.0\n",
       "0.8038745628679124,0.804367059438565,0.8060614952086994\n",
       "0.0,0.0,0.0,1.3345789403443214E-5,0.0,2.669157880688643E-5,0.0,4.003736821032964E-5,0.0,5.338315761377286E-5,0.0,8.007473642065928E-5,0.0,1.0676631522754571E-4,0.0,1.2011210463098892E-4,0.0,1.3345789403443215E-4,0.0,1.8684105164820498E-4,0.0,2.001868410516482E-4,0.0,2.1353263045509143E-4,0.0,2.669157880688643E-4,0.0,2.936073668757507E-4,0.0,3.2029894568263713E-4,0.0,3.3364473508608035E-4,0.0,3.603363138929668E-4,0.0,3.7368210329640996E-4,0.0,3.870278926998532E-4,0.0,4.40411050313...\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Record output\n",
    "var output=\"\"\n",
    "output+=acc_sup_arr.mkString(\",\")\n",
    "output+=\"\\n\"\n",
    "output+=roc_sup_arr.flatten.flatten.mkString(\",\")\n",
    "output+=\"\\n\"\n",
    "output+=auroc_sup_arr.mkString(\",\")\n",
    "output+=\"\\n\"\n",
    "var a = 0;\n",
    "for(a <- 0 until 11){\n",
    "    output+=roc_unsup_arr(a).flatten.mkString(\",\")\n",
    "    output+=\"\\n\"\n",
    "}\n",
    "output+=auroc_unsup_arr.mkString(\",\")\n",
    "output+=\"\\n\"\n",
    "output+=rfModel.featureImportances.toString\n",
    "\n",
    "Files.write(Paths.get(\"link_pred_output.txt\"), output.getBytes(StandardCharsets.UTF_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
